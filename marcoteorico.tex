\chapter{MARCO TEÓRICO}\label{chap:marco}
\vskip 3.0ex

En este capítulo se presentan las definiciones de grafos, cliques maximales, y algunas métricas de clusterización, junto a codificaciones ocupadas en este trabajo.

\section{Grafos, cliques maximales y métricas de clusterización} \label{marco:grafoclique}
Se define un \textbf{grafo} $G = (V, E)$ como el conjunto finito de \textit{vértices} o \textit{nodos} $V$ y el conjunto de \textit{aristas} $E \subseteq V \times V$ (arcos). La expresión $V(G)$ representa el conjunto de sus vértices y $E(G)$ el conjunto de sus aristas. El \textit{orden} de un grafo corresponde al total de sus vértices $|V(G)|$, mientras que el \textit{tamaño} de un grafo corresponde al total de sus aristas $|E(G)|$. 

Dos vértices $v_{1}$ y $v_{2} \in V(G)$ son \textbf{adyacentes} o \textbf{vecinos} si $(v_{1}, v_{2}) \in E(G)$ y $v_{1} \neq v_{2}$.  Un grafo es \textbf{no dirigido} cuando la arista conlleva ambos sentidos, quiere decir que $(v_{1}, v_{2}) = (v_{2}, v_{1})$, ambos vértices son vecinos directos entre sí. Distintos son los grafos \textbf{dirigidos}, donde las aristas tienen un solo sentido, y $(v_{1}, v_{2}) \neq (v_{2}, v_{1})$. En este caso, $v_{2}$ es llamado \textbf{vecino directo} de $v_{1}$, mientras $v_{1}$ es llamado \textbf{vecino inverso o reverso} de $v_{2}$. Un \textbf{grafo denso} es aquel que su número de aristas es cercano al máximo. Este trabajo está enfocado a utilizar grafos no dirigidos y poco densos. 

El \textbf{grado de un vértice} $d(v)$ se define como la cantidad de vértices en $V(G)$ que son adyacentes con $v$. La \textbf{matriz de adyacencia} de un grafo $G$ corresponde a una matriz binaria cuadrada $|V(G)| \times |V(G)|$ donde una celda $(i, j)$ almacena un 1 solo si existe una arista entre los vértices que corresponden a la conjunción del par (fila, columna) $= (i, j)$. En caso contrario, la celda contiene un 0.

\input{figs/bipartito}

Un grafo \textbf{k-degenerate} es un grafo no dirigido donde cada subgrafo tiene un vértice con grado a lo más \textbf{k}. El índice de \textbf{degeneracy} de un grafo, $D(G)$, es el menor valor \textbf{k} para el cual el grafo es \textbf{k-degenerate}.

Un grafo es \textbf{bipartito} cuando sus vértices se pueden separar en dos conjuntos separados $U$ y $W$, tal que se cumple $U \cup W = V$ y $U \cap W = \varnothing$. Un grafo es \textbf{bipartito completo} o \textbf{biclique}, cuando todos los vértices de un conjunto son vecinos directos de todos los vértices del otro conjunto. En la \autoref{fig:bipartito} se ilustra un ejemplo de grafo bipartito y un biclique.


Un \textbf{clique} es un subgrafo donde todos los vértices son adyacentes entre sí, es decir, $\exists V' \subseteq V(G), \forall v_{1}, v_{2} \in V', (v_{1}, v_{2}) \in E(G) $. Un \textbf{clique maximal} no puede extenderse incluyendo otro vértice adyacente, es decir, no es subconjunto de otro clique más grande. En la \autoref{fig:maxCliqueExample} se presenta ejemplo de un grafo y sus cliques maximales.

\input{figs/maxCliqueExample}

Un \textbf{triángulo} es un subgrafo de tres vértices y tres aristas. Se define $\lambda(v)$ como la cantidad de triángulos donde participa un nodo $v$, y $\lambda(G)$ como la cantidad de triángulos de un grafo, y se calcula sumando el cálculo individual para cada vértice, y dividiendo el total en tres (por cada triángulo se cuentan 3 veces los vértices), como lo muestra la siguiente ecuación

\begin{equation}
	\lambda(G) = \dfrac{1}{3} \sum_{v \in V} \lambda(v) \label{eq:triangles}
\end{equation}

Un \textbf{triplete} es un subgrafo de tres vértices y dos aristas, donde las aristas comparten un vértice común. Se define $\tau(v)$ como la cantidad de tripletes donde $v$ es el vértice común, y $\tau(G)$ como la cantidad de tripletes de un grafo.

\begin{equation}
	\tau(G) = \sum_{v \in V} \tau(v) \label{eq:triplets}
\end{equation}

El \textbf{coeficiente de clusterización} de un vértice indica cuánto está conectado con sus vecinos, y se define como $c(v) =  \lambda(v) / \tau(v)$. El coeficiente de clusterización de un grafo ($C(G)$) es el promedio del coeficiente de todos los nodos del grafo, y se define como:

\begin{align}
	C(G) &= \dfrac{1}{|V'|} \sum_{v \in V'} c(v) \label{eq:CC} \\
	V' &= \{ v \in V | d(v) \geq 2 \} \nonumber
\end{align}

\noindent donde $V'$ es el conjunto de vértices con un grado mayor a dos. Su rango es entre $[0, 1]$, mientras más cercano a $1$ indica más conexión entre vértices.

La \textbf{transitividad} de un grafo ($T(G)$) es la probabilidad que un par de nodos adyacentes estén interconectados, y se define como:

\begin{equation}
	T(G) = \dfrac{3 \lambda(G)}{\tau(G)} \label{eq:T} 
\end{equation}

\noindent y su rango también va entre $[0, 1]$, siendo $1$ cuando todos los nodos están interconectados con todos.

Tanto el coeficiente de clusterización como la transitividad son métricas que permiten vislumbrar cuán conectados o clusterizados están los vértices de un grafo, y de sus ecuaciones se puede notar que están relacionados.

\section{Codificaciones}

Existen distintos tipos de codificaciones, según la aplicación. En esta sección se resumen algunas de relevancia para este trabajo, como algunos códigos universales o la codificación Huffman.

\subsection{Códigos universales}\label{sec:Ucoding}
Los códigos universales para enteros son un tipo de códigos que transforman enteros positivos en secuencias de bits, donde el largo de la secuencia final de bits tiene relación con el entero a codificar. Existen varios códigos de este tipo, algunos son:

\begin{itemize}
	\item \textbf{Código unario}: Se representa un entero $x$ por una secuencia de $1^{x-1}0$, donde el $0$ indica el término de la secuencia. Por ejemplo, el número $5$ se representa por la secuencia $111110$. Este código no es muy eficiente por si solo, pero se usa de base para otro tipo de códigos.
	
	\item \textbf{Código gamma ($\gamma$)}: Se representa un entero $x$ en un par concatenado de \textit{largo} y \textit{offset}. \textit{Offset} es la representación binaria de $x$, pero sin el primer 1. Por ejemplo, para $x=5$ su representación binaria es $101$, por tanto su \textit{offset} es $01$. \textit{Largo} codifica el largo de \textit{offset} en código unario. Para $x=5$, el largo de \textit{offset} es 2 bits, por tanto \textit{largo} es $110$. Concatenando ambas, el código $\gamma$ para $x=5$ es $11001$.
	
	\item \textbf{Código delta ($\delta$)}: Este código es una extensión del código $\gamma$ para enteros largos. Básicamente hacen lo mismo, pero el \textit{largo} lo representan en código $\gamma$ en vez de código unario. El código $\delta$ para $x=5$ es $10001$.
\end{itemize}

\subsection{Codificación Huffman}\label{sec:huffman}
La codificación Huffman\cite{huffman1952method} es un técnica de compresión de datos óptima que define códigos de largo variable libre de prefijos. Es óptima porque el tamaño de la representación comprimida es mínima y es libre de prefijos porque ningún código es prefijo de otro. Huffman es un algoritmo greedy basado en definir códigos mas cortos para aquellos elementos mas frecuentes. 

Una codificación binaria de largo fijo, le asigna la misma cantidad de bits a todos los símbolos por codificar. Una de largo variable le asigna menos bits a los símbolos más frecuentes, y más bits a los menos frecuentes, cuidando que las secuencias binarias cortas no sean prefijos de las más largas. La \autoref{fig:fixedVarLength} se tiene la frecuencia de seis símbolos en una secuencia de 100.000 caracteres, y se comparan ambos códigos. Para el caso de largo fijo, se requieren 3 bits por cada símbolo en la secuencia, un total de 300.000 bits. Para el caso de largo variable, el símbolo más frecuente \texttt{a} requiere un bit, y los menos frecuentes requieren 4 bits. Así, la secuencia requiere:

\begin{align*}
	(45 \cdot 1 + 13 \cdot 3 + 12 \cdot 3 + 16 \cdot 3 + 9 \cdot 4 + 5 \cdot 4) \cdot \textrm{1.000} = \textrm{224.000 bits}
\end{align*}

\noindent lo que significa una reducción cercana al $20\%$. 

Un código prefijo es aquel donde ninguna palabra codificada es usada como prefijo de otra. Estos códigos son simples de decodificar, basta con comenzar el proceso desde el primer bit hasta encontrar una de las posibles codificaciones, traducirla a su valor original, y seguir con el resto de bits codificados. Siguiendo el ejemplo, la secuencia $001011101$ se identifica como $0 \cdot 0 \cdot 101 \cdot 1101$, y se traduce como \texttt{aabe}.

Para agilizar la búsqueda, el código prefijo se puede representar como un árbol binario, donde los nodos hojas son los caracteres codificados. Siguiendo cada bit de la secuencia, se puede ir avanzando por el árbol hasta llegar a un nodo hoja, y así llegar al valor buscado. En la \autoref{fig:fixVarTrees} se ilustran los árboles de las codificaciones de la \autoref{fig:fixedVarLength}, en (a) el correspondiente a código de largo fijo, y en (b) el de largo variable.

\input{figs/fixedVarLength}

\input{figs/fixVarTrees}

La codificación Huffman aprovecha todo lo anterior, utilizando una heurística \textit{greedy} para la construcción de su estructura y su compresión final. Asumiendo que se tiene una secuencia $C$ de $n$ caracteres, y que cada carácter $c \in C$ tiene una frecuencia $f[c]$ en $C$. El algoritmo crea un árbol desde los nodos hojas hacia el nodo raíz, inicialmente con $|C|$ hojas, y ejecutando $|C| - 1$ conexiones para llegar al árbol final. Luego identifica los dos elementos menos frecuentes y los conecta a un nuevo elemento, con frecuencia igual a la suma de ambos. Esto continúa hasta que todos los nodos hojas están conectados al árbol. En la \autoref{fig:huffman2} se ilustra este proceso para la secuencia ejemplo de la \autoref{fig:fixVarTrees}(b), y a continuación se detalla el proceso:

\begin{enumerate}
	\item \textbf{\autoref{fig:huffman2}(a)}: Se crean los $|C| = 6$ nodos hoja para cada carácter.
	\item \textbf{\autoref{fig:huffman2}(b)}: Se identifican los dos nodos de caracteres menos frecuentes, $f$ con $f[f] = 5$ y $e$ con $f[e] = 9$ (en miles), y se conectan a un nuevo nodo con frecuencia $5 + 9 = 14$.
	\item \textbf{\autoref{fig:huffman2}(c)}: Los siguientes nodos menos frecuentes son $c$ con $f[c] = 12$ y $b$ con $f[b] = 13$, y se conectan a otro nodo nuevo con frecuencia $12 + 13 = 25$.
	\item \textbf{\autoref{fig:huffman2}(d)}: El nodo creado que conecta $f$ con $e$ posee la menor frecuencia ($14$), y junto al nodo $d$ con $f[d] = 16$ se conectan en un nuevo nodo con frecuencia $14 + 16 = 30$.
	\item \textbf{\autoref{fig:huffman2}(e)}: Para juntar a los nuevos nodos de menor frecuencia, $25$ y $30$ respectivamente, se crea un nuevo nodo con frecuencia $25 + 30 = 55$.
	\item \textbf{\autoref{fig:huffman2}(f)}: Finalmente, se conecta el último nodo hoja restante, $a$ con $f[a] = 45$, con el reciente nodo creado de frecuencia $55$, mediante el nodo raíz con frecuencia $45 + 55 = 100$, confirmando la correcta creación del árbol.
\end{enumerate}

Se visualiza que se crearon $|C| - 1 = 6 - 1 = 5$ nodos para conectar todo el árbol. Con este resultado, se puede reconstruir de manera secuencial una secuencia codificada, simplemente recorriendo el árbol desde el nodo raíz hasta llegar a un nodo hoja, sustituir esa subsecuencia de bits por el valor de dicho nodo, y continuar con el resto de la secuencia de igual manera hasta el final. Retomando el ejemplo de secuencia $001011101$, en la \autoref{fig:huffmanBack} se visualizan los tres casos a decodificar: en (a) los primeros bits $0$ llegan al nodo hoja de \texttt{a}, en (b) la secuencia $101$ llega al nodo \texttt{b}, y en (c) la secuencia $1101$ llega al nodo \texttt{e}, dando en (d) la equivalencia entre bits y caracteres con resultado \texttt{aabe}.

\input{figs/huffman2}

\input{figs/huffmanBack}
